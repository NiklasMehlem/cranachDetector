\chapter{Recherche}
%
Ziel dieses Kapitel ist es, die Funktionsweise der verschiedenen Gesichtserkennungsmodelle grob zu erläutern, um ihre Ergebnisse besser einordnen zu können. Dabei wird bewusst auf eine vertiefte technische Beschreibung verzichtet, da dies den Rahmen dieser Arbeit überschreiten würde. Des Weiteren haben erste Stichprobentests gezeigt, dass die Modelle bei Bildern historischer Werke messbar andere Ergebnisse liefern als bei realen Fotos.

\begin{itemize}
	\item \textbf{Haar-Cascade:} Der \textbf{Haar-Cascade-\gls{detektor}} basiert auf dem von Viola und Jones entwickelten Verfahren zur schnellen Objekterkennung mittels einer kaskadierten \gls{boosting}-Architektur \parencite{ViolaJones2001}. Haar-ähnliche Merkmale werden ermittelt, indem die Differenz der Pixel-Summen benachbarter rechteckiger Regionen berechnet wird. Mithilfe eines Integralbilds lassen sich diese Merkmale in konstanter Zeit auswerten. Anschließend erfolgt die Merkmalsauswahl durch den AdaBoost-Algorithmus, der eine Vielzahl schwacher \gls{klassifikator}en zu einem starken \gls{klassifikator} kombiniert. Die \gls{klassifikator}en sind in mehreren Stufen kaskadiert, wobei jede Stufe unpassende Bildbereiche frühzeitig verwirft, um die Erkennungszeit zu verkürzen. In der ursprünglichen Implementierung umfasste die Kaskade 38 Stufen, beginnend mit nur einem Merkmal in der ersten Stufe und steigend auf bis zu 6000 Merkmale in späteren Stufen. 
In dieser Arbeit wird OpenCV verwendet, um mit der Klasse \texttt{CascadeClassifier} das vortrainierte XML-Modell zu laden. Das verwendete Modell \texttt{haarcascade\_frontalface\_default.xml} wurde mit dem \textbf{Caltech Frontal Face Dataset} von 1999 trainiert \parencite[101–102]{howse2019opencv}.
%
	\item \textbf{Caffe:} \textbf{\gls{caffe}} ist ein Deep-Learning-Framework, das zur Entwicklung und Ausführung von Modellen dient \parencite{JiaSDKLGGD14}. In dieser Arbeit wird das Modell \monofett{res10\_300x300\_ssd\_iter\_140000.caffemodel} zusammen mit \texttt{deploy.prototxt} verwendet, um die Parameter der Netzwerkarchitektur zu definieren. Wie der Modellname bereits andeutet, basiert es auf dem \gls{ssd}-Framework mit einem \textbf{ResNet10}-\gls{backbone}. \gls{ssd}-Modelle bestehen aus einem \gls{ssd}-Head und einem \gls{backbone} \parencite{Esri2025SSD}. Häufig wird ein Netzwerk wie ResNet verwendet, wobei die finale Klassifikationsebene entfernt wird, um daraus das \gls{backbone} zu erzeugen. \gls{ssd} nutzt mehrere Feature-Maps, auf denen sogenannte Default-Boxes platziert werden, welche auf Merkmale untersucht werden \parencite{Liu2016}. Die Angabe \enquote{300x300} im Modellnamen zeigt an, dass das Modell mit Bildern der Größe 300 × 300 Pixel trainiert wurde. Damit das Modell auch größere Eingaben verarbeiten kann, wird die Funktion \texttt{blobFromImage} von OpenCV genutzt, um das Bild für die Gesichtserkennung auf 300 × 300 zu skalieren \parencite{sefiks14451}. Die Trainingsdaten des Modells sind nicht öffentlich dokumentiert
%	
	\item \textbf{MediaPipe:} \textbf{MediaPipe} ist ein Open-Source-Framework von Google, das zur Erstellung von On-Device-Machine-Learning-Pipelines entwickelt wurde \parencite{mediapipeFramework, mediapipeSolutions}. MediaPipe bietet neben der Gesichtserkennung auch weitere Funktionen wie Hand- und Körpererkennung. In dieser Arbeit wird ausschließlich das Teilmodul \textit{MediaPipe Face Detection} betrachtet, welches ein Teil von \textit{MediaPipe Solutions} ist. Als Ergebnis liefert MediaPipe sowohl die Position des Gesichts als auch die von Gesichtsmerkmalen wie Augen, Nasenspitze und Mund \parencite{mediapipeFace}. \textit{MediaPipe Face Detection} nutzt \textbf{BlazeFace} als Modell in seiner Pipeline, welches auf 128 × 128 Pixel trainiert wurde \parencite{mediapipeFace}. \textit{BlazeFace} verwendet ein \gls{ssd}-Framework, das für eine schnellere Performanz modifiziert wurde, sodass eine Geschwindigkeit von 200 bis zu 1000 FPS erreicht werden kann. Es wurde explizit für den Einsatz auf mobilen Endgeräten optimiert und geht daher effizienter mit GPU-Ressourcen um \parencite{Bazarevsky2019}.
%	
	\item \textbf{Dlib HOG:} Dlib ist ein Open-Source-C++-Toolkit, das unter anderem Gesichtserkennung mit vorgefertigten Methoden ermöglicht \parencite{dlib2022}. Die hier verwendete Methode \texttt{\seqsplit{get\_frontal\_face\_detector}} nutzt den \gls{hog}. \gls{hog} ist ein Feature-Deskriptor, der Bilder in \gls{featurevektor}en überführt, indem er lokale Gradienten berechnet, daraus Histogramme erstellt und diese blockweise normalisiert \parencite{1467360}. Dlib kombiniert \gls{hog} mit einem Sliding-Window-Verfahren über eine \gls{bildpyramide} sowie einer \gls{svm} \parencite{trainObjectDetectoroD, faceDetectoroD}. Beim Sliding-Window-Verfahren wird mit einer festen Fenstergröße über das Bild iteriert und jeder so erzeugte Bildausschnitt analysiert \parencite{Esri2025SSD}. Die \gls{svm} dient als \gls{klassifikator}, um die vom \gls{hog} erzeugten \gls{featurevektor}en aus den verschiedenen Bildausschnitten zu bewerten und festzustellen, ob diese Gesichter enthalten \parencite{1467360}.
%	
	\item \textbf{Dlib HOG Landmark:} Verwendet wird in dieser Arbeit das \monofett{shape\_predictor\_68\_face\_landmarks.dat}-Modell von Davis King \parencite{King24}. Das Modell basiert auf der Methode aus dem Paper ``One Millisecond Face Alignment with an Ensemble of Regression Trees'' von \cite{Kazemi2014} \parencite{faceLandmarkDetectionoD}. Darin wird gezeigt, wie Gesicht-Landmarks innerhalb von Millisekunden mittels Regressionsbäumen und \gls{boosting} geschätzt werden können. Wie im Namen abzulesen ist, markiert das Modell 68 Landmarks in einer vordefinierten \gls{boundingbox} \parencite{DlibPythonoD}. Landmarks sind Punkte im Gesicht wie Augen, Nase und Lippen \parencite{faceLandmarkDetectionoD}. Das Modell verwendet \gls{boundingbox}es und markiert in diesen die Landmarks.
Es braucht zusätzlich die Hilfe eines \gls{detektor}s, der die benötigten \gls{boundingbox}es zur Verfügung stellen kann. Im Fall von \monofett{shape\_predictor\_68\_face\_landmarks.dat} wurde dieses darauf ausgelegt, zusammen mit Dlib \gls{hog} verwendet zu werden, weswegen in dieser Arbeit Dlib \gls{hog} für die Erstellung der \gls{boundingbox}es verwendet wird \parencite{King24}. Das Modell wurde mithilfe des iBUG 300-W-Datensatzes trainiert \parencite{King24, faceLandmarkDetectionoD}.
%
	\item \textbf{Dlib CNN:} \monofett{mmod\_human\_face\_detector.dat} ist ein Modell von Dlib, das ein \gls{cnn} verwendet. Dlib selbst beschreibt das Modell als genauer im Vergleich zu \gls{hog}, weist aber auch darauf hin, dass die höhere Genauigkeit auf Kosten eines höheren Rechenaufwands und damit verbundener Latenz erreicht wird \parencite{cnnFaceDetectoroD}. Wie aus dem Namen des Modells ersichtlich, wurde es mithilfe von \gls{mmod} trainiert. \gls{mmod} ist ein Trainingsverfahren, bei dem alle möglichen Sub-Fenster in einem Bild berücksichtigt werden – im Gegensatz zum Sliding-Window-Verfahren, das nur über Stichproben möglicher Fensteroptionen iteriert \parencite{King15}. Ein \gls{cnn} erkennt Gesichter, indem es das Eingabebild schrittweise mit kleinen Matrizen, sogenannten Filtern oder Kernels, faltet \parencite{OSheaN15}. Dabei gleiten die Filter über das Bild und erzeugen durch die Faltung für jeden Filter eine eigene Feature-Map. Anschließend wird Pooling angewendet, welches benachbarte Bereiche zusammenfasst und so die Auflösung sowie den Rechenaufwand reduziert \parencite{OSheaN15, GeeksforGeeks25}. Durch die Beibehaltung der zweidimensionalen Struktur kann ein \gls{cnn} räumliche Zusammenhänge im Bild ausnutzen und so Merkmale wie Kanten zuverlässig erkennen.
%	
	\item \textbf{MTCNN:} \textbf{\gls{mtcnn}} kombinieren Gesichtserkennung und Gesichtsausrichtung in einem Modell \parencite{ZhangZL016}. Dafür werden drei \gls{cnn}s kaskadiert, um sowohl effizient als auch effektiv zu arbeiten. Das erste \gls{cnn} wird \gls{pnet} genannt \parencite{ZhangZL016}. Das \gls{pnet} ist ein schnelles, weniger tiefes \gls{cnn}, das mithilfe einer \gls{bildpyramide} die ersten möglichen \gls{boundingbox}es markiert. Anschließend wird \gls{nms} verwendet, um überlappende \gls{boundingbox}es zusammenzufügen. Im zweiten Schritt werden weitere falsche \gls{boundingbox}es, die keine Gesichter enthalten, vom sogenannten \gls{rnet} verworfen. Dieses ist ein komplexeres \gls{cnn} im Vergleich zum \gls{pnet} \parencite{ZhangZL016}. Zuletzt wird das stärkste \gls{cnn} eingesetzt, nachdem nur noch wenige \gls{boundingbox}es verbleiben. Das \gls{onet} markiert fünf Landmark-Punkte und generiert so das Endergebnis \parencite{ZhangZL016}.
%	
	\item \textbf{Yunet:} \textbf{Yunet} ist ein leichtgewichtiger \gls{detektor} und \gls{cnn}, das speziell für den Einsatz auf ressourcenbeschränkten Geräten wie Smartphones entwickelt wurde \parencite{wu2023yunet}. Anders als andere \gls{detektor}en wie \gls{ssd} nutzt Yunet einen ankerfreien Ansatz \parencite{wu2023yunet}. Indem auf \gls{anker} oder Default-Boxes verzichtet wird, benötigt Yunet weniger Rechenleistung. Allerdings haben ankerfreie Ansätze Probleme mit Bildern, die kleine oder Gesichtern unterschiedlicher Größen zeigen \parencite{Wang2018}.
%	
	\item \textbf{RetinaFace (CPU):} \textbf{RetinaFace} ist ein einstufiger, pixelweiser Gesichtsdetektor, welcher je nach Variante ResNet oder MobileNet als \gls{backbone} verwendet \parencite{abs-1905-00641}. Einstufig bedeutet, dass Klassifikation und Lokalisierung in einem einzigen Durchlauf gemeinsam ausgeführt werden, ähnlich wie beim \gls{ssd}. Pixelweise heißt hier, dass für jede Gitterzelle der Feature-Map eine Vorhersage getroffen wird. Pro Zelle gibt RetinaFace einen Face-Score, eine \gls{boundingbox}, fünf Landmark-Koordinaten sowie zusätzliche Informationen für ein 3D-Mesh des Gesichts aus \parencite{abs-1905-00641}. Um Gesichter verschiedener Größen zuverlässig zu erkennen, nutzt RetinaFace Feature-Pyramiden, bei denen Feature-Maps unterschiedlicher Auflösungen kombiniert werden. Trainiert wurde das Modell auf dem \textbf{WIDER FACE}-Datensatz \parencite{abs-1905-00641}. Zusätzlich zu klassischen Trainingsansätzen wurden auf allen erkennbaren Gesichtern des Datensatzes fünf Landmark-Punkte händisch annotiert, um die Lokalisierungsgenauigkeit zu erhöhen. Ein zusätzlicher Zweig des Modells erzeugt außerdem ein 3D-Gesichts-Mesh, welches für selbstüberwachtes Lernen verwendet wird \parencite{abs-1905-00641}.
\end{itemize}
