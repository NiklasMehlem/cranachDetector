\chapter{Recherche}

Nicht alle Gesichtserkennungsmodelle sind in der Lage überhaupt zuverlässig Gesichtern in alten Gemälden zu erkennen. Bevor also zu viel Zeitaufwand entsteht alle möglichen Modelle intensiv zu testen und mit einander zu vergleichen werden ihnen Stichprobenartig jeweils 4 Bilder vorgelegt um einzuschätzen ob diese sich für weitere Test eignen.

Recherche dient um Grob Idee zu geben und damit man weiß womit man eigentlich Arbeitet. Es soll nicht ins Detail gegeangen werden da diese den Ramen der Arbeit sprengen würde.

ERKLÄRE WIE DIE STICHPROBEN TESTS FUNKTIONIEREN

BILD MAße Normal und "Hochauflösendend"

\section{Haar-Cascade}
Der \textbf{Haar‑Cascade‑Detektor} basiert auf dem von Viola und Jones entwickelten Verfahren der schnellen Objekterkennung mittels einer kaskadierten Boosting-Architektur \parencite{ViolaJones2001}. Dabei werden Haar‑ähnliche Merkmale erfasst, die als Differenz der Pixel‑Summen benachbarter rechteckiger Regionen berechnet werden. Mithilfe eines Integralbilds lassen sich diese Merkmale dann in konstanter Zeit evaluieren. Anschließend erfolgt die Merkmalsauswahl durch den AdaBoost‑Algorithmus, welcher eine Vielzahl schwacher Klassifikatoren zu einem starken Klassifikator kombiniert. Die Klassifikatoren sind in mehreren Stufen kaskadiert, wobei jede Stufe unpassende Bildbereiche frühzeitig verwirft um schneller ein Ergebnis zu erzielen. In der ursprünglichen Implementierung umfasste die Kaskade 38 Stufen, beginnend mit nur einem Merkmal in der ersten Stufe und steigend bis zu 6000 Features in späteren Stufen. Die Implementierung in dieser Arbeit erfolgt über OpenCV, welche mit der Klasse \textbf{CascadeClassifier} vortrainierte XML‑Modelle lädt. Trainiert wurde das verwendete \monofett{haarcascade\_frontalface\_default.xml} Modell mit dem \textbf{Caltech Frontal Face Dataset} von 1999 \parencite[101–102]{howse2019opencv}.

Haar-Cascade, genauer \monofett{haarcascade\_frontalface\_default.xml}, eignet sich in keiner Weise. Es scheitert sowohl darin zuverlässig das Gesicht in einem Portrait zu erkennen, als auch darin mehrere Gesichter in einem Gruppen Bild zu erkennen. Zudem kommt es oft zu false-positives. Teilweise werden dadurch mehr false-positives als tatsächliche Gesichter markiert. Haar Cascade hat nicht direkt einen 'confidence' Wert den man bearbeiten kann aber ein äquivalent, jedoch ist selbst mit verschiedenen Werten Haar Cascade nicht in der Lage zuverlässige Ergebnisse zuliefern und scheidet damit bereits hier für weitere Versuche aus.
Anzumerken ist ebenfalls das Haar-Cascade bei gleichen Motiv mit höherer Auflösung noch schlechter abschneidet, in dem es zu mehr false-positives kommt.

\section{Caffe}
\gls{caffe} ist ein Deep‑Learning‑Framework das zur Entwicklung und Ausführung von Modellen dient \parencite{JiaSDKLGGD14}. Verwendet wird in dieser Arbeit \monofett{res10\_300x300\_ssd\_iter\_140000.caffemodel} zusammen mit \monofett{deploy.prototxt} zur Definierung der Parameter der Netzwerkarchitektur. Wie der Name des Modells hergibt basiert es auf dem \gls{ssd} Framework mit einem \textbf{ResNet10}-Backbone. \gls{ssd}s bestehen aus einem \gls{ssd}-Head und einem Backbone \parencite{Esri2025SSD}. Dabei ist es oft so das ein Netzwerk wie ResNet genommen wird und die finale Klassifikations-Ebene entfernt wird um daraus das Backbone zu erstellen. \gls{ssd} nutzt mehrere Feature-Maps um auf diesen Default-Boxes zu platzieren, welche auf Merkmale untersucht werden \parencite{Liu2016}. Die 300x300 im Namen des Modells geben an, dass das Modell mit Bildern mit 300 x 300 Pixelgröße trainiert wurde. Damit das Modell größere Eingaben verarbeiten kann wird die \texttt{blobFromImage} von OpenCV genutzt um das Bild für die Gesichtserkennung auf 300 x 300 zu skalieren \parencite{sefiks14451}. Genaue Trainingsdaten für das Modell wurden leider nicht Dokumentiert.

\gls{caffe} besteht den Stichproben Test. In 1er, 2er und 3er Portraits schafft es jeweils alle 'Hauptgesichter' zu erkennen. Zusätzlich ist es dabei auch sehr sicher ohne ein einziges false-positiv bei allen Stichproben Tests. Allerdings scheint es dafür mit kleineren Gesichtern, oder Gesichtern die nicht Hauptfokus des Portraits sind Probleme zu haben. So wurde im Gruppen Bild nicht ein Gesicht markiert, auch kein false-positiv. Reduziert man den confidence Wert auf 0.2 so ist \gls{caffe} auch in der Lage alle Gesichter im 3er Portrait zu erkennen, weiter ohne false-positives. Das Gruppenbild hingegen bleibt der Schwachpunkt des Modells. Um Gesichter im Gruppenbild zu erkennen muss der confidence Wert so weit reduziert werden, dass es zu so vielen false-positives kommt das die Ergebnisse unbrauchbar werden. Verwendet man Bilder mit höherer Auflösung so werden gleiche Ergebnisse in den Stichproben Tests erzielt. Lediglich beim Gruppenbild kommt es zu mehr False-Positives. Einen großen Unterschied macht dies allerdings nicht, da das \gls{caffe} Modell schon zuvor Schwierigkeiten mit diesem Bild hatte. Verwendet man Bilder mit geringer Auflösung so schafft es \gls{caffe} weiter alle Gesichter zu erkennen ohne False-Positves, wenn auch im 3er Portrait ein Nebengesicht nur halb getroffen wurde. Überraschender weiße wurden im Gruppenbild mit geringer Auflösung tatsächlich Bereiche maskiert. Auch wenn dabei einige Gesichter korrekt markiert wurden, waren die markierten Bereiche viel zu groß, als das sie sinnvoll wären. Überraschenderweise als der Stichproben-Test mit Bildern mittlerer Auflösung (600 x 1130 Pixel) wiederholt wurde wurde im 3er Portrait das Nebengesicht nicht erkannt, obwohl es bei größeren und kleineren Bild Größen zuvor erkannt wurde. Diese inkonsistent wird es schwerer machen eine geeignete Bildgröße für die Tests zu finden, oder es könnte ein Grund werden warum \gls{caffe} am Ende doch verworfen wird.

\section{MediaPipe}
\textbf{MediaPipe} ist ein open-source Framework von Google, das zur Erstellung von on-device machine learning pipelines entwickelt wurde \parencite{mediapipeFramework, mediapipeSolutions}. MediaPipe ist in der Lage mehr als nur Gesichtserkennung zu machen, in dieser Arbeit wird sich der Einfachheit nur auf MediaPipe Face Detection fokussiert, welches ein Teil von MediaPipe Solutions ist. Als Ergebnis liefert MediaPipe sowohl die Position des Gesichts als auch die von Gesichtsmerkmalen wie Augen, Nasenspitze und Mund \parencite{mediapipeFace}. MediaPipe Face Detection nutzt \textbf{BlazeFace} als Modell in seiner Pipeline, welches auf 128 x 128 Pixel trainiert wurde. \parencite{mediapipeFace}. BlazeFace verwendet ein \gls{ssd} Framework das auf schnellere Performanz modifiziert wurde, so dass eine Geschwindigkeit von ca. 200-1000 FPS erreicht werden kann. Es wurde explizit entwickelt um mit Mobile-GPUs zu Arbeiten und effizienter mit GPU Ressourcen umzugehen \parencite{Bazarevsky2019}.

MediaPipe schafft es beim 1er Portrait das Gesicht sicher zu erkennen ohne false-positives. Dies ist aber auch der einzige Stichproben Test den MediaPipe besteht. Bei allen anderen Tests wird von MediaPipe nichts markiert, weder Gesichter noch false-positives. Das lässt vermuten, dass MediaPipe nur in der Lage ist große Gesichter zu erkennen. Da dieses Modell sehr ineffektiv für den gewünschten Kontext scheint wird es hier aussortiert und wird nicht weiter getestet. In Stichproben Tests mit höher auflösenden Bildern erzielt MediaPipe Identische Ergebnisse.

\section{Dlib HOG}
\textbf{Dlib} ist ein open source C++-Toolkit, welches unter anderem Gesichtserkennung mit vorgefertigten Methoden ermöglicht \parencite{dlib2022}. Die hier verwendete \texttt{\seqsplit{get\_frontal\_face\_detector}} nutzt den \gls{hog}. \gls{hog} ist ein Feature-Deskriptor, welcher Bilder zu Feature-Vektoren verarbeitet indem er lokale Gradienten berechnet und aus diesen Histogramme erstellt und diese blockweise normalisiert \parencite{1467360}. Dlib kombiniert \gls{hog} mit einem Sliding-Window-Verfahren über eine Bildpyramide und einer \gls{svm} \parencite{trainObjectDetectoroD, faceDetectoroD}. Beim Sliding-Window-Verfahren fährt man mit einer festen Fenstergröße über das Bild und analysiert jeden so erzeugten Bildausschnitt \parencite{Esri2025SSD}. Die \gls{svm} dient als Klassifikator um die von \gls{hog} erzeugten Feature-Vektoren aus den verschiedenen Bildausschnitten zu bewerten um festzustellen ob diese Gesichter enthalten \parencite{1467360}.

Das \gls{hog} basierte Gesichtsdetektionsmodell von dlib zeigt in den Stichproben Tests, dass es gut darin ist kleine Gesichter zu erkennen aber Probleme mit größeren hat. So hat es sehr große Probleme das Gesicht beim 1er Portrait zu erkennen, ohne dass man den confidence Wert ins unbrauchbare reduziert. Je nach confidence Wert kommt es auch noch zu kleinen Mengen (ca. 0 bis 3) von false-positives. Doch wurden die Gesichter im 2er, 3er und auch größten Teils im Gruppenbild gut erkannt. Das Modell weißt zwar offensichtliche Schwäche auf, allerdings sind die Ergebnisse gut genug das es weiter getestet wird. Es könnte sich als nützliche alternative für Bilder herausstellen bei denen andere Modelle Probleme haben die Gesichter zu erkennen. Verwendet man hochauflösende Bilder in den Stichproben Tests so schafft es \gls{hog} mit gleicher confidence das Gesicht im 1er Portrait zu erkennen, allerdings erhöht sich die Anzahl an false-positives. Die Ergebnisse erinnern somit an jene mit geringerem confidence wert bei Bildern mit geringere Auflösung. Es scheint so als müsste der confidence Wert von \gls{hog} in abhängigkeit der Bildauflösung eingestellt werden. Sollte dies der Fall sein könnte es das Modell deutlich unattraktiver durch den erhöhten Aufwand in der Verwendung machen. Bei einem weiteren Test mit Bildern geringer Auflösung ergibt sich beim 1er Portrait weiter nur ein false-positiv, dafür keine False-Positives bei den 2er und 3er Portraits. Jedoch gibt es beim Gruppen Bild eine deutliche Verschlechterung aus 6 erkannten Gesichtern und 3 False-Positives wurden 1 erkanntes Gesicht und 2 False-Positives. Zugegeben durch die geringe Auflösung sind die Gesichter des Gruppen Bilds relativ schwer erkennbar. Daran wird gezeigt das Portraits und Gruppenbilder unterschiedlich behandelt werden sollten und sich fürs erste weiter auf Portraits fokussiert werden sollte. Testet man Dlib \gls{hog} mit Bildern mittlerer Auflösung, so werden in allen Bildern, bis auf dem Gruppenbild, alle Gesichter erkannt. Dabei kam es in jedem Bild auch zu 1-2 False-Positives.

\section{Dlib HOG Landmark}
Verwendet wird in dieser Arbeit das \monofett{shape\_predictor\_68\_face\_landmarks.dat} Modell von Davis King \parencite{King24}. Das Modell basiert auf der Methode aus dem Paper ''One Millisecond Face Alignment with an Ensemble of Regression Trees'' von \cite{Kazemi2014} \parencite{faceLandmarkDetectionoD}. Darin wird gezeigt wie Gesicht-Landmarks innerhalb von Millisekunden mittels Regressionsbäumen und Boosting geschätzt werden können. Wie im Namen abzulesen ist markiert das Modell 68 Landmarks in einer vordefinierten Bounding-Box \parencite{DlibPythonoD}. Landmarks sind dabei unter anderem Punkte im Gesicht wie Augen, Nase und Lippen \parencite{faceLandmarkDetectionoD}. Das Modell nimmt Bounding-Boxes und markiert in diesen die Landmarks. 
%Dabei wird nicht überprüft ob diese überhaupt Gesichter enthalten, sind keine Gesichter enthalten werden dennoch Landmarks platziert. 
Es braucht zusätzlich die Hilfe eines Detektors der die benötigten Bounding-Boxes zur Verfügung stellen kann. Im Fall von \monofett{shape\_predictor\_68\_face\_landmarks.dat} wurde dieses darauf ausgelegt zusammen mit Dlib \gls{hog} verwendet zu werden, weswegen in dieser Arbeit Dlib \gls{hog} für die Erstellung der Bounding-Boxes verwendet wird \parencite{King24}. Das Modell wurde mithilfe des iBUG 300-W Datensatzes trainiert \parencite{King24, faceLandmarkDetectionoD}.

Vor der Recherche wurde davon ausgegangen das das Landmark Modell seine Eingaben überprüft, so das genauere Ergebnisse im Vergleich zum einfachen Dlib \gls{hog}. Dies scheint nicht der Fall zu sein, allerdings können die markierten Landmarks analysiert werden um so von Dlib \gls{hog} makierte false-positives auszusortieren. Mit diesem Ansatz wird Dlib \gls{hog} Landmark in den Stichproben-Tests untersucht ob es tatsächlich genauer sein kann.
Da in dieser Arbeit Dlib \gls{hog} verwendet wurde um die Bounding-Boxes für das Landmark Modell zu bestimmen sind die Ergebnisse beider Modelle identisch. Das Landmark Modell hat dabei keine Probleme die Landmarks der identifizierten Gesichter korrekt zu platzieren. Die markierten Gesichter wurden nun noch einmal überprüft ob false-positives auszusortieren für genauere Ergebnisse im Vergleich zu Dlib \gls{hog} ohne Landmark. Allerdings wurde dabei meist die korrekten Gesichter aussortiert statt false-positives. Es wurde auch false-positives aussortiert, was jedoch irrelevant ist wenn das Modell Gesichter nur schlecht erkennt. Dlib \gls{hog} Landmark wird daher nicht weiter getestet, da es im Vergleich zu Dlib \gls{hog} keinen Mehrwert liefert. Hochauflösende Bilder liefern dabei auch keine besseren Ergebnisse, sondern in den meisten Fällen eher mehr flase-positives. Im 3er Portrait wurde zwar 1 Gesicht mehr erkannt, was jedoch immer noch deutlich Schlechter als einfaches Dlib \gls{hog} ist, welches alle Gesichter im 3er Portrait erkannt hat.

%Ähnlich wie das Dlib HOG Modell zuvor hat Dlib HOG Landmark, in dieser Arbeit \monofett{shape\_predictor\_68\_face\_landmarks.dat}, Probleme damit große Gesichter zu erkennen. Es schneidet gleich schlecht ab wie Dlib HOG beim 1er Portrait. Es ist bei den anderen Tests allerdings genauer gewesen und hatte weniger false-positives. Somit wird Dlib HOG Landmark ebenfalls weiter getestet. Das Dlib HOG Landmark mit kleineren Gesichtern besser funktioniert zeigt sich weiter in den Stichproben Tests mit hochauflösenden Bildern. So kam es nicht teils zu mehr false-positives sondern auch dazu das die confidence erkannter Gesichter geringer wurde im Vergleich zu den Tests mit niedrigerer Auflösung. Bei kleinen Gesichtern wie im 3er Portrait hingegen wurde die confidence geringfügig besser oder blieb gleich. Somit sollten die weiter genau beobachtet werden wie konstant Dlib HOG Landmark ist, und ob es sich weiter für den Kontext eignet.

\section{Dlib CNN}
\monofett{mmod\_human\_face\_detector.dat} ist ein Modell von Dlib, welches \gls{cnn} verwendet. Dlib selbst beschreibt das Modell als genauer im Vergleich zu \gls{hog}, informiert aber auch das die höhere Genauigkeit auf kosten höherer benötigter Rechenleistung und damit verbundener Latenz kommt \parencite{cnnFaceDetectoroD}. Wie aus dem Namen des Modells zu entnehmen wurde es mithilfe von \gls{mmod} trainiert. \gls{mmod} ist ein Trainingsverfahren bei dem alle möglichen Sub-Fenster in einem Bild berücksichtigt werden, anders als beim Sliding-Window-Verfahren wo nur über Stichproben von möglichen Fensteroptionen iteriert wird \parencite{King15}. Ein \gls{cnn} erkennt Gesichter, indem es das Eingabebild schrittweise mit kleinen Matrizen, genannt Filtern oder Kernels, faltet \parencite{GeeksforGeeks25}. Dabei gleiten die Filter über das Bild und erzeugen durch die Faltung für jeden Filter eine eigene Feature-Map. Anschließend wird Pooling angewendet, das jeweils benachbarte Bereiche zusammenfasst und so die Auflösung sowie den Rechenaufwand reduziert \parencite{GeeksforGeeks25}. Durch die Beibehaltung der zweidimensionalen Struktur kann ein \gls{cnn} räumliche Zusammenhänge im Bild ausnutzen und so Merkmale wie Kanten zuverlässig erkennen.

Dlib \gls{cnn}, genauer \monofett{mmod\_human\_face\_detector.dat}, ist das spürbar langsamste Modell von allen bisher getesteten. In der Dokumentation wurde bereits über eine hohe Latenz berichtet, allerdings ist diese deutlich spürbarer als erwartet \parencite{cnnFaceDetectoroD}. Bereits bei Bildern der Größe 998 x 1314 Pixel muss für ca. 20 Sekunden auf ein Ergebnis gewartet werden, während die anderen Modelle unter einer Sekunde bleiben. Das Ergebnis der Stichproben Tests ist dafür Fehlerlos. Bei 1er, 2er und 3er Portraits wurden alle Gesichter erkannt ohne false-positives. Nur beim Gruppenbild wurde kein Gesicht oder false-positiv erkannt. Mit eine so zuverlässigen Ergebnis wird Dlib \gls{cnn} weiter getestet und verglichen. Am ende muss sich zeigen, dass die deutlich längere Bearbeitungszeit sich auch in deutlich besserer Zuverlässigkeit widerspiegelt um sie zu rechtfertigen. Verwendet man hochauflösende Bilder, hier 1200 x 1593 Pixel, so bleiben die Ergebnisse gleich. Lediglich die Warte Zeit erhöht sich deutlich. Aus ca. 20 Sekunden Bearbeitungszeit wurden ca. 40. Es handelt sich hierbei zwar nur um Stichproben, allerdings könnte dies der große Nachteil von \gls{cnn} sein wenn bei bereits 20\% größeren sich die Bearbeitungszeit verdoppelt wenn bereits die Ursprüngliche Bearbeitungszeit deutlich über dem durchschnitt der anderen getesteten Modelle liegt. Verwendet man Bilder mit geringer Auflösung ca. 400 x 531 Pixel so arbeitet das Modell mit vergleichbarer Geschwindigkeit verglichen mit den anderen Modellen. Allerdings war es dann nur noch in der Lage Gesichter im 1er Portrait zu erkennen, auf allen anderen Bildern wurde nichts mehr markiert. Bei Stichprobentests mit Bildern mittlerer Auflösung, werden wieder alle Gesichter im 1er, 2er und 3er Portrait gefunden bis auf das Nebengesicht im 3er Portrait. Anders als bei den meisten anderen Modellen die getestet wurden, scheint Dlib \gls{cnn} mit größeren Bildern besser zu arbeiten. Nachteil bleibt das dadurch die Bearbeitungszeit weiter hoch bleibt um von Dlib \gls{cnn}s Genauigkeit zu nutzen.

\section{MTCNN}
\gls{mtcnn} kombiniert Gesichtserkennung und Gesichtsausrichtung in einem Modell \parencite{ZhangZL016}. Dafür werden 3 \gls{cnn}s kaskadiert um sowohl effizient als auch effektiv zu Arbeiten. Das erste \gls{cnn} wird \gls{pnet} genannt \parencite{ZhangZL016}. Das \gls{pnet} ist ein schnelles oberflächliches \gls{cnn}, das mithilfe einer Bildpyramide die ersten möglichen Bounding-Boxes markiert. Anschließend wird \gls{nms} verwendet um überlappende Bounding-Boxes zusammen zufügen. Im zweiten Schritt werden weiter falsche Bounding-Boxes vom sogenannten \gls{rnet} verworfen die keine Gesichter enthalten, welches ein komplexeres \gls{cnn} im Vergleich zum \gls{pnet} ist \parencite{ZhangZL016}. Nun da es nur noch eine kleine Anzahl von Bounding-Boxes gibt, wird das stärkste \gls{cnn}, das \gls{onet}, eingesetzt welches fünf Landmark Punkte markiert und damit das Endergebnis generiert \parencite{ZhangZL016}.

\gls{mtcnn} hat den Stichproben Test für 1er, 2er und 3er Portraits fehlerlos bestanden. Beim Gruppen Bild wurden bloß ein paar Gesichter erkannt, dabei sollten nur Ergebnisse mit > 0.5 confidence makiert werden. Es gab des weiteren nicht einen false-positiv. Somit wird \gls{mtcnn} sicher weiter getestet. Verwendet man Bilder mit höherer Auslösung so zeigt sich im Stichproben-Test das die confidence erkannter Gesichter im schnitt gleich bleibt, allerdings kommt es bei manchen Bildern zu 1 bis 3 false-positives. Im Gruppen Bild hingegen wurden ohne weitere false-positives bei höherer Auflösung mehr Gesichter erkannt, wenn auch nur 6 von 13 insgesamt. Die false-positives bei höherer Auflösung haben sehr hohe confidence Werte (ca. 0.85) was das bestimmen eines besserer geeigneten confidence Wertes erschwert. In den kommenden Tests muss sich zeigen ob \gls{mtcnn} weiter verlässlich bleibt, oder ob sich doch false-positives unvermeidlich dazu mischen. Werden Bilder mit geringer Auflösung getestet so erkennt \gls{mtcnn} weiter alle Gesichter in 1er, 2er und 3er Portraits korrekt markiert noch dazu mit 1.0 confidence. Weiter gibt es keine False-Positves, und im Gruppen Bild werden keine Bereiche markiert. In Stichprobentests mit mittlerer Auflösung bleiben die Ergebnisse größtenteils Identisch, nur wurden im Gruppenbild 4 Gesichter erkannt mit ca. 0.85 confidence.

\section{Yunet}
Yunet ist ein leichtgewichtiger Detektor und \gls{cnn} der speziell für den Einsatz auf ressourcenbeschränkten Geräten wie Smartphones entwickelt wurde \parencite{wu2023yunet}. Anders als Detektoren wie \gls{ssd}, nutzt Yunet einen ankerfreien Ansatz \parencite{wu2023yunet}. Dadurch das ohne Anker oder Default-Boxes gearbeitet wird, benötigt Yunet weniger Rechenleistung für seine Arbeit. Allerdings haben ankerfreie Ansätze Probleme mit kleineren Gesichtern oder wenn es in einem Bild unterschiedlich große Gesichter gibt \parencite{Wang2018}.

Das Yunet Modell, in diesem fall \monofett{face\_detection\_yunet\_2023mar.onnx}, ist ohne weitere Einstellungen sehr streng. Bei den Stichproben Test kam es zu keinen false-positives, allerdings wurde in manchen Bildern nicht alle Gesichter markiert. Nachdem der confidence Wert auf 0.8 eingestellt wurde, wurde Problemlos alle Gesichter des 1er, 2er und 3er Portraits erkannt. Aus dem Gruppen Bild wurden nur wenige Gesichter markiert. Ob 0.8 der optimale Wert ist wird sich zeigen wenn Yunet intensiver getestet wird. Aufgrund seiner Genauigkeit wird Yunet weiter getestet. Nutzt man Bilder mit höherer Auflösung in den Stichproben Tests so verbessert sich der confidence Wert der gefundenen Gesichter. Beim Gruppenbild wurde ein zuvor erkanntes Gesicht nicht erkannt, ein vorher nicht erkanntes Gesicht dann wiederum schon. Weiter gab es bei keinem der Stichproben Tests mit höherer Auflösung ein false-positiv, womit Yunet ein sicherer Kandidat bleibt für die kommenden Tests. Im Stichprobentest mit Bildern geringer Auflösung findet Yunet weiter alle Geischter in allen Bildern bis auf im Gruppenbild wo es keines mehr erkennt. Werden Bildern mit mittlerer Auflösung verwendet so bleiben die Ergebnisse Identisch zu denen aus den Test geringer Auflösung.	

\section{RetinaFace (CPU)}
RetinaFace ist ein einstufiger, pixelweiser Gesichtsdetektor, welcher je nach Variante ResNet oder MobileNet als Backbone verwendet \parencite{abs-1905-00641}. Einstufig bedeutet, dass Klassifikation und Lokalisierung in einem einzigen Durchlauf gemeinsam ausgeführt werden, ähnlich wie beim \gls{ssd}. Pixelweise bedeutet hier, dass für jede Gitterzelle der Feature-Map eine Vorhersage getroffen wird.
Pro Zelle gibt RetinaFace einen Face-Score, eine Bounding-Box, fünf Landmark-Koordinaten sowie zusätzliche Informationen für ein 3D-Mesh des Gesichts aus \parencite{abs-1905-00641}. Um Gesichter verschiedener Größen zuverlässig zu erkennen, nutzt RetinaFace Feature-Pyramiden, bei denen Feature-Maps unterschiedlicher Auflösungen kombiniert werden.
Trainiert wurde das Modell auf dem \textbf{WIDER FACE} Datensatz \parencite{abs-1905-00641}. Zusätzlich zu klassischen Trainingsansätzen wurden auf allen erkennbaren Gesichtern des Datensatzes fünf Landmark-Punkte händisch annotiert, um die Lokalisierungsgenauigkeit zu erhöhen. Ein zusätzlicher Zweig des Modells erzeugt außerdem ein 3D-Gesichts-Mesh, welches für selbst überwachtes Lernen verwendet wird \parencite{abs-1905-00641}.

Verwendet wird in dieser Arbeit RetinaFace mit ResNet-50 Backbone und CPU Einsatz. Grund dafür ist, dass RetinaFace mit GPU Einsatz lediglich schneller und nicht genauer sein soll. So bleiben die Werte von RetinaFace später vergleichbarer. Die Option das mit einer geeigneten GPU schnellere Ergebnisse erzielt werden können, kann später bei der Auswertung noch berücksichtigt werden. Was die Ergebnisse des Stichproben Tests angeht hat RetinaFace das beste Ergebnisse aller Modelle bisher. Alle Gesichter ohne false-positives im 1er, 2er und 3er Portrait erkannt. Noch dazu wurden 10 der 13 Gesichter im Gruppenbild erkannt, wo die besten Ergebnisse bisher ca. 6 Gesichter waren. Jedoch sei anzumerken das es 2 false-positives gab bei denen die Gesichter von Pferden markiert wurden. Ob RetinaFace diese Quote beibehalten kann wird sich in den folgenden Tests ergeben. RetinaFace bleibt bei den Stichproben Tests mit höherer auflösenden Bildern konstant. Confidence Werte verbessern sich leicht oder verschlechtern sich um wenige 0.05 Punkte. Weiter gibt es auch keine neuen false-positives. Lediglich beim Gruppenbild wurde ein Gesicht weniger erkannt, dieses hatte aber auch schon im Test zuvor den geringsten confidence Wert von allen erkannten Gesichtern. Im Test mit Bildern mit geringer Auflösung schneidet RetinaFace weiter am besten ab. Es gab keine False-Positives und es wurden alle Gesichter im 1er, 2er und 3er Portrait erkannt. Die Anzahl erkannter Gesichter sankt zwar auf 8 von 13 im Gruppenbild, ist aber bei weitem das beste Ergebnis mit dieser Auflösung wo andere Modelle meist nichts mehr erkannt haben.
